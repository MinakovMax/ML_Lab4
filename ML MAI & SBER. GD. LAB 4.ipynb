{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"-7QbAnE-T5CZ","pycharm":{"name":"#%% md\n"}},"source":["### О задании\n","\n","В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n","\n","Баллы даются за выполнение отдельных пунктов (Максимальное количество баллов за эту Л.Р. - 5)\n","\n","Задачи в рамках одного раздела рекомендуется решать в том порядке, в котором они даны в задании.\n","\n","Неэффективная реализация кода может негативно отразиться на оценке.\n","Также оценка может быть снижена за плохо читаемый код и плохо считываемые диаграммы.\n","\n","Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n","\n","\n","### Формат сдачи\n","Задания сдаются через lms. Вы прикрепляете **ССЫЛКУ НА ПУБЛИЧНЫЙ РЕПОЗИТОРИЙ**, где выполнено ваше задание.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Lrv2xhEZT5Cb","pycharm":{"name":"#%% md\n"}},"source":["Приготовьтесь, потому что на лекциях и семинарах мы уже прошли через джунгли теории, оттачивая наши навыки в мастерстве оптимизации функционалов. Мы погружались в глубины градиентного спуска, изучая его в каждом возможном амплуа — от классического полного градиента до беспощадного стохастического градиента, не забывая про метод с импульсом, который как боксерский удар прорывается сквозь проблемы оптимизации.\n","\n","Теперь же перед вами стоит вызов, который не для слабонервных. Ваша миссия, если вы, конечно, осмелитесь ее принять, — взять в арсенал четыре разнообразных вида градиентного спуска и смастерить из них инструмент, способный расправиться с любой задачей. Вам предстоит создать собственную версию линейной регрессии, такую, что даже самые опытные аналитики данных будут смотреть на нее с завистью. Испытайте на реальных данных весь арсенал вашего градиентного спуска, сравните, какой из них выходит на арену оптимизации как несокрушимый чемпион.\n","\n","Это задание не для тех, кто привык стоять в сторонке. Это ваш момент славы, ваш шанс выйти на арену, где вашим оружием будет код, а противниками — самые коварные задачи машинного обучения. Покажите, на что вы способны, и пусть ваш код станет легендой!\n","\n","@GPT-4"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vYJ6DgC1T5Cb","pycharm":{"name":"#%% md\n"}},"source":["## Задание 1. Реализация градиентного спуска (1.75 балла)\n","\n","В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле `descents.py`.\n","\n","**Все реализуемые методы должны быть векторизованы!**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fcQobFLwT5Cc","pycharm":{"name":"#%% md\n"}},"source":["### Лирическое размышление № 1\n","\n","Ключевая характеристика антиградиента заключается в том, что он направлен к самому быстрому уменьшению значения функции в конкретной точке. Исходя из этого, разумным подходом будет начать движение с определенной точки, переместиться в направлении антиградиента, затем вновь вычислить антиградиент, совершить движение и продолжать таким образом. Давайте опишем этот процесс более формализованно.\n","\n","Предположим, что $w_0$ – это исходный набор параметров (к примеру, набор из нулей или полученный из какого-либо случайного распределения). В этом случае простой градиентный спуск предполагает выполнение следующих действий до достижения сходимости:\n","\n","$$\n","    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n","$$"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Kqp8PYhcT5Cc","pycharm":{"name":"#%% md\n"}},"source":["### Лирическое размышление № 2\n","\n","### Задание 0.0. Градиент MSE в матричном виде (0 баллов).\n","\n","Напомним, что функция потерь MSE записывается в матричном виде как:\n","\n","Давайте найдем градиент функции $Q(w)$ по $w$, используя матричное дифференцирование. Функция $Q(w)$ определена как:\n","\n","$$\n","Q(w) = \\frac{1}{\\ell} (y - Xw)^T (y - Xw)\n","$$\n","\n","где:\n","- $Q(w)$ — функция потерь,\n","- $\\ell$ — количество наблюдений\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aWR1zIF0T5Cc"},"source":["- $y$ — вектор истинных значений,\n","- $X$ — матрица признаков,\n","- $w$ — вектор весов.\n","\n","Градиент функции потерь $Q(w)$ по $w$ находится следующим образом:\n","\n","1. Раскроем скобки в выражении для $Q(w)$:\n","   \n","$$\n","Q(w) = \\frac{1}{\\ell} (y^Ty - y^TXw - w^TX^Ty + w^TX^TXw)\n","$$\n","\n","2. Заметим, что $y^TXw$ и $w^TX^Ty$ представляют собой скаляры и равны между собой. Тогда выражение упрощается до:\n","   \n","$$\n","Q(w) = \\frac{1}{\\ell} (y^Ty - 2y^TXw + w^TX^TXw)\n","$$\n","\n","3. Теперь дифференцируем $Q(w)$ по $w$. При дифференцировании $y^Ty$ как константа относительно $w$ исчезает, а дифференциация оставшейся части дает:\n","\n","$$\n","\\nabla_w Q(w) = \\frac{1}{\\ell} (-2X^Ty + 2X^TXw)\n","$$\n","\n","4. Упростим выражение, вынеся 2 за скобки:\n","\n","$$\n","\\nabla_w Q(w) = \\frac{2}{\\ell} (X^TXw - X^Ty)\n","$$\n","\n","Таким образом, градиент функции потерь $Q(w)$ по вектору весов $w$ равен:\n","\n","$$\n","\\nabla_w Q(w) = \\frac{2}{\\ell} (X^TXw - X^Ty)\n","$$\n","\n","Это выражение и есть искомый градиент."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mMWb49rzT5Cc","pycharm":{"name":"#%% md\n"}},"source":["### Задание 1.1. Родительский класс BaseDescent (0.25 балла).\n","\n","Реализуйте функции `calc_loss` (вычисление MSE для переданных $x$ и $y$) и `predict` (предсказание $y_{pred}$ для переданных $x$) в классе `BaseDescent`.\n","\n","Все вычисления должны быть векторизованы."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pvvK9aQaT5Cc","pycharm":{"name":"#%% md\n"}},"source":["### Задание 1.2. Полный градиентный спуск VanillaGradientDescent (0.25 балла).\n","\n","Реализуйте полный градиентный спуск заполнив пропуски в классе `VanillaGradientDescent` в файле `descents.py`. Для вычисления градиента используйте формулу выше. Шаг оптимизации:\n","\n","$$\n","    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n","$$\n","\n","Здесь и далее функция `update_weights` должна возвращать разницу между $w_{k + 1}$ и $w_{k}$: $\\quad w_{k + 1} - w_{k} = -\\eta_{k} \\nabla_{w} Q(w_{k})$.\n","\n","Во всех методах градиентного спуска мы будем использовать следующую формулу для длины шага:\n","\n","$$\n","    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n","$$\n","\n","На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_57oF08vT5Cc","pycharm":{"name":"#%% md\n"}},"source":["### Лирическое размышление № 3\n","\n","Конечно, давайте переформулируем и перепишем ваш текст для лучшего понимания.\n","\n","В контексте задач машинного обучения, обычно функционал ошибки $Q(w)$ можно представить как среднее арифметическое отдельных ошибок на каждом элементе выборки:\n","\n","$$\n","    Q(w) = \\frac{1}{\\ell} \\sum_{i = 1}^{\\ell} q_i(w),\n","$$\n","\n","где каждая функция $q_i(w)$ отражает ошибку на i-ом объекте выборки.\n","\n","Основная сложность применения метода градиентного спуска заключается в необходимости вычисления градиента по всей выборке на каждом шаге. Это может быть особенно затруднительно при работе с большими данными. Однако, для эффективного шага в направлении минимизации функции потерь, абсолютная точность градиента может быть не столь критична.\n","\n","Мы можем приблизить градиент всей функции, используя среднее значение градиентов для случайно выбранной подвыборки объектов:\n","\n","$$\n","    \\nabla_{w} Q(w_{k}) \\approx \\dfrac{1}{|B|} \\sum_{i \\in B} \\nabla_{w} q_{i}(w_{k}),\n","$$\n","\n","где $B$ является подмножеством выборки с случайно выбранными индексами.\n","\n","Этот подход приводит нас к методу **стохастического градиентного спуска**, который значительно упрощает вычисления и ускоряет процесс обучения, особенно на больших данных."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wP2WVO_oT5Cd","pycharm":{"name":"#%% md\n"}},"source":["### Задание 1.3. Стохастический градиентный спуск StochasticDescent (0.25 балла).\n","\n","Реализуйте стохастический градиентный спуск заполнив пропуски в классе `StochasticDescent`. Для оценки градиента используйте формулу выше (среднее градиентов случайно выбранного батча объектов). Шаг оптимизации:\n","\n","$$\n","    w_{k + 1} = w_{k} - \\eta_{k} \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}).\n","$$\n","\n","Размер батча будет являться гиперпараметром метода, семплируйте индексы для батча объектов с помощью `np.random.randint`."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iaau6dMYT5Cd","pycharm":{"name":"#%% md\n"}},"source":["### Лирическое размышление № 4\n","\n","В процессе оптимизации может случиться так, что направление наискорейшего спуска, определенное антиградиентом, будет резко колебаться от одного шага к другому. Это часто происходит, если функция потерь имеет вытянутые уровни, что приводит к тому, что градиент, всегда перпендикулярный этим линиям, меняет свое направление на противоположное при каждом шаге. Эти колебания могут серьезно замедлить сходимость оптимизационного процесса из-за постоянных \"колебаний\" в обратных направлениях. Чтобы сгладить эти осцилляции и ускорить процесс оптимизации, применяется метод усреднения градиентов из нескольких предыдущих шагов, тем самым снижая \"шум\" и выявляя общее предпочтительное направление движения. Это достигается с помощью введения вектора инерции:\n","\n","\\begin{align}\n","    &h_0 = 0, \\\\\n","    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k}),\n","\\end{align}\n","\n","где $\\alpha$ — коэффициент, контролирующий влияние градиентов предыдущих шагов, уменьшая их вклад со временем. Можно использовать аппроксимацию градиента для вычисления $h_{k + 1}$. Для осуществления следующего шага градиентного спуска текущую точку смещают на вектор инерции:\n","\n","$$\n","    w_{k + 1} = w_{k} - h_{k + 1}.\n","$$\n","\n","Такой подход позволяет сгладить колебания градиента: если градиент по какому-то направлению часто меняет знак, его вклад в вектор инерции будет уменьшаться, в то время как постоянное направление градиента приведет к увеличению шага в этом направлении, делая процесс оптимизации более стабильным и направленным."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4UWMm3VUT5Cd","pycharm":{"name":"#%% md\n"}},"source":["### Задание 1.4 Метод Momentum MomentumDescent (0.25 балла).\n","\n","Реализуйте градиентный спуск с методом инерции заполнив пропуски в классе `MomentumDescent`. Шаг оптимизации:\n","\n","\\begin{align}\n","    &h_0 = 0, \\\\\n","    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k}) \\\\\n","    &w_{k + 1} = w_{k} - h_{k + 1}.\n","\\end{align}\n","\n","$\\alpha$ будет являться гиперпараметром метода, но в данном домашнем задании мы зафиксируем её за вас $\\alpha = 0.9$."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ligtCpjmT5Cd","pycharm":{"name":"#%% md\n"}},"source":["### Лирическо размышление № 5\n","\n","Выбор размера шага играет критическую роль в эффективности градиентного спуска. Слишком большой шаг может привести к тому, что процесс будет \"перепрыгивать\" минимальное значение, а слишком маленький шаг существенно замедлит достижение минимума, требуя большего количества итераций. Предварительно определить идеальный размер шага невозможно, и даже стратегии постепенного его уменьшения могут оказаться неэффективными.\n","\n","AdaGrad предлагает индивидуальный подход к регулированию длины шага для каждой отдельной компоненты параметров. Суть метода заключается в уменьшении размера шага в зависимости от общей длины предыдущих шагов по данному параметру:\n","\n","\\begin{align}\n","    &G_{kj} = G_{k-1,j} + (\\nabla_w Q(w_{k - 1}))_j^2; \\\\\n","    &w_{jk} = w_{j,k-1} - \\frac{\\eta_t}{\\sqrt{G_{kj}} + \\varepsilon} (\\nabla_w Q(w_{k - 1}))_j.\n","\\end{align}\n","\n","Где $\\varepsilon$ — малая добавка для предотвращения деления на ноль.\n","\n","В AdaGrad размер шага может быть фиксирован с самого начала, исключая необходимость его подбора в процессе. Этот метод особенно эффективен в задачах с разреженными данными, где большинство признаков для объектов равны нулю. Таким образом, большие шаги будут совершаться по редко встречающимся признакам, в то время как по часто встречающимся — маленькие.\n","\n","Основной недостаток AdaGrad заключается в неизбежном замедлении шагов из-за монотонного роста $G_{kj}$, что может остановить процесс до достижения оптимального решения. Эту проблему решает метод RMSprop, где применяется экспоненциальное сглаживание для градиентов:\n","\n","$$\n","    G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2.\n","$$\n","\n","Здесь шаг адаптируется в зависимости от интенсивности движения по каждому направлению на недавних итерациях.\n","\n","Объединяя идеи этих методов, можно достичь эффективного накопления информации о градиентах для стабилизации процесса и внедрить адаптивную длину шага для каждого параметра, обеспечивая более сбалансированное и быстрое приближение к минимуму."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iYFN8nzBT5Cd","pycharm":{"name":"#%% md\n"}},"source":["### Задание 1.5. Метод Adam (Adaptive Moment Estimation) (0.75 балла).\n","\n","Реализуйте градиентный спуск с методом Adam заполнив пропуски в классе `Adam`. Шаг оптимизации:\n","\n","\\begin{align}\n","    &m_0 = 0, \\quad v_0 = 0; \\\\ \\\\\n","    &m_{k + 1} = \\beta_1 m_k + (1 - \\beta_1) \\nabla_w Q(w_{k}); \\\\ \\\\\n","    &v_{k + 1} = \\beta_2 v_k + (1 - \\beta_2) \\left(\\nabla_w Q(w_{k})\\right)^2; \\\\ \\\\\n","    &\\widehat{m}_{k} = \\dfrac{m_k}{1 - \\beta_1^{k}}, \\quad \\widehat{v}_{k} = \\dfrac{v_k}{1 - \\beta_2^{k}}; \\\\ \\\\\n","    &w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\widehat{v}_{k + 1}} + \\varepsilon} \\widehat{m}_{k + 1}.\n","\\end{align}\n","\n","$\\beta_1 = 0.9, \\beta_2 = 0.999$ и $\\varepsilon = 10^{-8}$ будут зафиксированы за вас."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XT2SZZ5dT5Cd","pycharm":{"name":"#%% md\n"}},"source":["## Задание 2. Реализация линейной регресии (0.25 балла)\n","\n","Ваша задача — создать собственную версию линейной регрессии, которая будет обучаться с помощью метода градиентного спуска, следуя предоставленным шаблонам в файле `linear_regression.py` под классом **LinearRegression**. Главные требования к реализации:\n","\n","- Используйте векторизацию для всех вычислений, минимизируйте использование циклов в Python, за исключением итераций градиентного спуска.\n","- Прекращайте обучение, когда выполнено хотя бы одно из следующих условий:\n","  - Евклидова норма разности векторов весов между двумя последовательными итерациями становится меньше заданного порога `tolerance`.\n","  - В векторе весов появляются значения NaN.\n","  - Достигнуто максимальное количество итераций `max_iter`.\n","- Предполагается, что данные для обучения уже содержат столбец из единиц в качестве последнего столбца, обеспечивающего вектор свободных членов.\n","- Для отслеживания процесса сходимости используйте массив `loss_history`, куда следует записывать значения функции потерь до начала первого шага градиентного спуска и после каждой итерации, включая итоговое значение после завершения обучения."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Tr-r2Q7CT5Cd","pycharm":{"name":"#%% md\n"}},"source":["## Задание 3. Проверка кода (0 баллов)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"AM1oq0kzT5Ce","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["%load_ext autoreload"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"i-lxiKJHT5Ce","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["%autoreload 2\n","\n","import numpy as np\n","\n","from descents import get_descent\n","from linear_regression import LinearRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWLILm38T5Cf","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["num_objects = 100\n","dimension = 5\n","\n","x = np.random.rand(num_objects, dimension)\n","y = np.random.rand(num_objects)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0YvNTWzT5Cf","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["\n","descent_config = {\n","    'descent_name': 'типа что-то делает',\n","    'kwargs': {\n","        'dimension': dimension\n","    }\n","}\n","\n","for descent_name in ['full', 'stochastic', 'momentum', 'adam']:\n","    descent_config['descent_name'] = descent_name\n","    descent = get_descent(descent_config)\n","\n","    diff = descent.step(x, y)\n","    gradient = descent.calc_gradient(x, y)\n","    predictions = descent.predict(x)\n","\n","    assert gradient.shape[0] == dimension, f'Gradient failed for descent {descent_name}'\n","    assert diff.shape[0] == dimension, f'Weights failed for descent {descent_name}'\n","    assert predictions.shape == y.shape, f'Prediction failed for descent {descent_name}'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEVnwJVvT5Cf","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# LinearRegression\n","\n","max_iter = 10\n","tolerance = 0\n","\n","descent_config = {\n","    'descent_name': 'stochastic',\n","    'kwargs': {\n","        'dimension': dimension,\n","        'batch_size': 10\n","    }\n","}\n","\n","regression = LinearRegression(\n","    descent_config=descent_config,\n","    tolerance=tolerance,\n","    max_iter=max_iter\n",")\n","\n","regression.fit(x, y)\n","\n","assert len(regression.loss_history) == max_iter, 'Loss history failed'"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"M6FLAkkyT5Cf","pycharm":{"name":"#%% md\n"}},"source":["## Задание 4. Работа с данными (0.5 балла)\n","\n","Мы будем использовать датасет объявлений по продаже машин на немецком Ebay. В задаче предсказания целевой переменной для нас будет являться цена."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xnvmIlbuT5Cf","pycharm":{"name":"#%% md\n"}},"source":["Вам нужно выполнить базовый EDA анализ:\n","\n","1. **Визуализация распределения целевой переменной**:\n","    - Постройте график распределения целевой переменной, чтобы оценить его форму.\n","    - Если распределение сильно скошено, рассмотрите возможность применения логарифмического преобразования к целевой переменной для нормализации распределения.\n","    - Оцените наличие выбросов, аномально высоких или низких значений целевой переменной, используя графический метод или статистические меры (например, интерквартильный размах).\n","\n","2. **Удаление выбросов**:\n","    - Если в данных присутствуют выбросы с аномальной ценой, удалите их, чтобы они не искажали результаты анализа и моделирования.\n","\n","3. **Исследование данных**:\n","    - Проанализируйте типы данных в столбцах (категориальные, числовые, текстовые и т.д.).\n","    - Постройте графики для анализа зависимости целевой переменной от других признаков. Это поможет понять, какие признаки влияют на целевую переменную.\n","    - Изучите распределения значений признаков для выявления аномалий и выбросов. Определите, какие признаки требуют предварительной обработки или трансформации.\n","    - На основе графиков и анализа определите, какие признаки кажутся полезными для моделирования.\n","\n","4. **Предобработка данных**:\n","    - Определите, какие трансформации данных (например, нормализация, стандартизация, кодирование категориальных переменных) могут быть применены к признакам.\n","    - Разделите признаки на категории: категориальные, числовые (вещественные) и те, которые не требуют предобработки.\n","\n","5. **Разделение данных на выборки**:\n","    - Разделите ваши данные на обучающую, валидационную и тестовую выборки в пропорции 8:1:1. Это важный шаг для оценки производительности модели и избежания переобучения.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eBGv6f8zT5Cf","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["%autoreload 2\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from descents import get_descent\n","from linear_regression import LinearRegression\n","\n","sns.set(style='darkgrid')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HISB4M-NT5Cg","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["data = pd.read_csv('autos.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Vh3Mh9gT5Cg","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data['price'].hist(bins=50)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["data['log_price'] = np.log(data['price'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data['log_price'].hist(bins=50)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["<Axes: >"]},"execution_count":7,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["data.boxplot('log_price')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["Q1 = data['log_price'].quantile(0.25)\n","Q3 = data['log_price'].quantile(0.75)\n","IQR = Q3 - Q1\n","    \n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","    \n","data = data[(data['log_price'] >= lower_bound) & (data['log_price'] <= upper_bound)]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Oz4SPbBgT5Cg","pycharm":{"name":"#%% md\n"}},"source":["Колонки в данных:\n","\n","* `brand` - название бренда автомобиля\n","* `model` - название модели автомобиля\n","* `vehicleType` - тип транспортного средства\n","* `gearbox` - тип трансмисcии\n","* `fuelType` - какой вид топлива использует автомобиль\n","* `notRepairedDamage` - есть ли в автомобиле неисправность, которая еще не устранена\n","* `powerPS` - мощность автомобиля в PS (метрическая лошадиная сила)\n","* `kilometer` - сколько километров проехал автомобиль, пробег\n","* `autoAgeMonths` - возраст автомобиля в месяцах\n","\n","\n","* `price` - цена, указанная в объявлении о продаже автомобиля (целевая переменная)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["brand_log_price = data.groupby('brand')['price'].mean()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","brand_log_price.plot(kind='bar')\n","plt.xlabel('Brand')\n","plt.ylabel('Average price')\n","plt.title('Average price by brand')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Группируем данные по 'brand' и 'model' и находим среднее значение 'price' для каждого бренда и модели\n","brand_model_price = data.groupby(['brand', 'model'])['price'].mean()\n","\n","# Создаем отдельный график для каждого бренда\n","brands = data['brand'].unique()\n","for brand in brands:\n","    brand_data = brand_model_price.loc[brand]\n","    plt.figure(figsize=(8, 5))\n","    brand_data.plot(kind='bar', color='skyblue')\n","    plt.xlabel('Model')\n","    plt.ylabel('Average price')\n","    plt.title(f'Average price by model for brand {brand}')\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Колонки в данных:\n","\n","* `brand` - название бренда автомобиля\n","* `model` - название модели автомобиля\n","* `vehicleType` - тип транспортного средства\n","* `gearbox` - тип трансмисcии\n","* `fuelType` - какой вид топлива использует автомобиль\n","* `notRepairedDamage` - есть ли в автомобиле неисправность, которая еще не устранена\n","* `powerPS` - мощность автомобиля в PS (метрическая лошадиная сила)\n","* `kilometer` - сколько километров проехал автомобиль, пробег\n","* `autoAgeMonths` - возраст автомобиля в месяцах\n","\n","\n","* `price` - цена, указанная в объявлении о продаже автомобиля (целевая переменная)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["brand_log_price = data.groupby('vehicleType')['price'].mean()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","brand_log_price.plot(kind='bar')\n","plt.xlabel('Тип транспортного средства')\n","plt.ylabel('Средняя цена')\n","plt.title('Средняя цена по типу транспортного средства')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["brand_log_price = data.groupby('gearbox')['price'].mean()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","brand_log_price.plot(kind='bar')\n","plt.xlabel('Тип трансмиссии')\n","plt.ylabel('Средняя цена')\n","plt.title('Средняя цена по типу трансмиссии')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["brand_log_price = data.groupby('fuelType')['price'].mean()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","brand_log_price.plot(kind='bar')\n","plt.xlabel('Вид топлива')\n","plt.ylabel('Средняя цена')\n","plt.title('Средняя цена по виду топлива')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["brand_log_price = data.groupby('notRepairedDamage')['price'].mean()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","brand_log_price.plot(kind='bar')\n","plt.xlabel('Неисправоность')\n","plt.ylabel('Средняя цена')\n","plt.title('Средняя цена по неисправности')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Строим график\n","plt.figure(figsize=(10, 6))\n","plt.scatter(data['powerPS'], data['price'], color='blue')\n","plt.xlabel('Мощность (powerPS)')\n","plt.ylabel('Цена')\n","plt.title('Линейная зависимость цены от мощности')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Строим график\n","plt.figure(figsize=(10, 6))\n","plt.scatter(data['kilometer'], data['price'], color='blue')\n","plt.xlabel('Пробег в км')\n","plt.ylabel('Цена')\n","plt.title('Линейная зависимость цены от пробега')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Строим график\n","plt.figure(figsize=(10, 6))\n","plt.scatter(data['autoAgeMonths'], data['price'], color='blue')\n","plt.xlabel('Возраст авто в месяцах')\n","plt.ylabel('Цена')\n","plt.title('Линейная зависимость цены от возраста')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Колонки в данных:\n","\n","* `brand` - название бренда автомобиля\n","* `model` - название модели автомобиля\n","* `vehicleType` - тип транспортного средства\n","* `gearbox` - тип трансмисcии\n","* `fuelType` - какой вид топлива использует автомобиль\n","* `notRepairedDamage` - есть ли в автомобиле неисправность, которая еще не устранена\n","* `powerPS` - мощность автомобиля в PS (метрическая лошадиная сила)\n","* `kilometer` - сколько километров проехал автомобиль, пробег\n","* `autoAgeMonths` - возраст автомобиля в месяцах\n","\n","\n","* `price` - цена, указанная в объявлении о продаже автомобиля (целевая переменная)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["data['brand_model'] = data['brand'].to_string() + '_' + data['model'].to_string()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Создание объекта StandardScaler\n","standart_scaler = StandardScaler()\n","\n","# Применение стандартизации к признаку 'powerPS'\n","data['powerPS_standardized'] = standart_scaler.fit_transform(data[['powerPS']])\n","\n","# Создание объекта MinMaxScaler\n","min_max_scaler = MinMaxScaler()\n","# Применение нормализации к признаку 'kilometer'\n","data['kilometer_normalized'] = min_max_scaler.fit_transform(data[['kilometer']])\n","\n","data['autoAgeMonths_normalized'] = min_max_scaler.fit_transform(data[['autoAgeMonths']])"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"0_MOBhTET5Cg","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["categorical = ['brand', 'model', 'brand_model', 'vehicleType', 'gearbox', 'fuelType', 'notRepairedDamage']\n","numeric = ['powerPS_standardized', 'kilometer_normalized', 'autoAgeMonths_normalized']\n","#other = []"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"L1PbGPzYT5Cg","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["#data['bias'] = 1\n","#other += ['bias']\n","\n","x = data[categorical + numeric]\n","y = data['log_price']"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"gAS9suGWT5Cg","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","column_transformer = ColumnTransformer([\n","    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n","    ('scaling', StandardScaler(), numeric)\n","#    ('other',  'passthrough', other)\n","])\n","\n","x = column_transformer.fit_transform(x)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"KLld6SSeT5Cg","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Сначала разделим на обучающую и временную выборки (80% и 20% соответственно)\n","X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","# Затем разделим временную выборку на валидационную и тестовую (10% и 10% от исходного датасета)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# В итоге у нас есть:\n","# X_train, y_train - обучающая выборка (80%)\n","# X_val, y_val - валидационная выборка (10%)\n","# X_test, y_test - тестовая выборка (10%)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mZUC1CaLT5Cg","pycharm":{"name":"#%% md\n"}},"source":["## Задание 5. Сравнение методов градиентного спуска (1 балл)\n","\n","В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Oj-CDtIAT5Cg","pycharm":{"name":"#%% md\n"}},"source":["### Задание 5.1. Подбор оптимальной длины шага (0.5 балла)\n","\n","Процесс выбора наиболее подходящего размера шага $\\lambda$ для различных методов, с учетом валидационного набора данных, предполагает выполнение следующих шагов:\n","\n","1. **Определение диапазона для $\\lambda$**: Начните с выбора диапазона значений $\\lambda$, используя логарифмическую сетку от $10^{-4}$ до $10^1$, чтобы обеспечить широкий охват потенциально оптимальных значений.\n","\n","2. **Перебор значений $\\lambda$**: Для каждого значения из выбранной сетки $\\lambda$:\n","   - Произведите обучение модели на обучающем наборе данных.\n","   - Вычислите ошибку на обучающем и валидационном наборах данных.\n","   - Определите значение метрики $R^2$ как на обучающем, так и на валидационном наборах.\n","   - Зафиксируйте количество итераций, необходимое для достижения сходимости.\n","\n","3. **Оценка полученных результатов**:\n","   - Составьте графики, отображающие зависимость ошибки от количества итераций для каждого значения $\\lambda$ по всем рассматриваемым методам.\n","   - Сравните методы на основе скорости сходимости, размера ошибки и значения метрики $R^2$ на различных наборах данных.\n","\n","4. **Выбор наилучшего $\\lambda$**: Исходя из проведенного анализа, определите наиболее подходящее значение $\\lambda$ для каждого метода, обеспечивающее оптимальное сочетание скорости сходимости и качества модели на валидационной выборке.\n","\n","5. **Формулировка выводов**: Подведите итоги, указав, какой метод показал наилучшую производительность с точки зрения соотношения скорости сходимости к качеству предсказаний. Также отметьте, как изменение $\\lambda$ влияет на результаты каждого из методов.\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"LzzkJ12eT5Ch","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from typing import Dict, List, Tuple\n","from sklearn.metrics import r2_score\n","\n","from descents import LossFunction\n","\n","lambda_values = np.logspace(-4, 1, num=5)\n","\n","max_iter = 500\n","tolerance = 1e-4\n","batch_size = 10\n","iter_count = 1500\n","\n","\n","# Преобразование разреженной матрицы в массив NumPy\n","X_train_array = X_train.toarray()\n","Y_train_array = y_train.values\n","\n","# Валидационная выборка\n","X_val_array = X_val.toarray()\n","y_val_array = y_val.values\n","\n","def print_params(lr, last_train_loss, val_loss, r2_train, r2_val, iter_count, mu: float = None, batch_sizes: int = None):\n","    \"\"\"\n","    Выводит в консоль информацию о лучших параметрах модели.\n","\n","    Этот метод предназначен для вывода значений ключевых метрик и параметров после процесса обучения модели, \n","    включая скорость обучения, ошибку на тренировочной и валидационной выборках, а также коэффициент детерминации R^2 \n","    на тренировочной и валидационной выборках. Параметр mu относится к коэффициенту регуляризации, а batch_sizes — к размеру батча.\n","    \"\"\"\n","\n","    base_message = (\n","        f'Параметры:\\n'\n","        f'lambda_ = {lr} \\n'\n","        f'Ошибка на тренировочной выборке = {last_train_loss} \\n'\n","        f'Ошибка на валидационной выборке = {val_loss} \\n'\n","        f'R2_train = {r2_train} \\n'\n","        f'R2_val = {r2_val} \\n'\n","        f'Iter_count = {iter_count}\\n'\n","    )\n","\n","    if mu is not None:\n","        base_message += f'mu = {mu} \\n'\n","\n","    if batch_sizes is not None:\n","        base_message += f'Batch_size = {batch_sizes} \\n'\n","    \n","    print(base_message)\n","\n","\n","def get_train_result(loss_function: LossFunction = LossFunction.MSE ,lr: float = 0.01, descent_name: str = 'adam', descent_reg: bool = False, isBasis: bool = True, mu: float = 0.1,  batch_size: int = None, tolerance: float = 1e-4, \n","                     max_iter: int = 500, min_loss: float = 0) -> Tuple[float, float, float, float, List[float]]:\n","    \"\"\"\n","    Обучает модель линейной регрессии, используя заданную конфигурацию градиентного спуска, и возвращает результаты.\n","\n","    Parameters:\n","    ----------\n","    lr : float, optional\n","        Скорость обучения (learning rate). По умолчанию равно 0.01.\n","    descent_name : str, optional\n","        Название метода градиентного спуска. Поддерживает 'adam' и 'stochastic'. По умолчанию 'adam'.\n","    batch_size : int, optional\n","        Размер батча для стохастического градиентного спуска. Только для 'stochastic'. По умолчанию None.\n","    tolerance : float, optional\n","        Критерий остановки для квадрата евклидова нормы разности весов. По умолчанию равен 1e-4.\n","    max_iter : int, optional\n","        Максимальное количество итераций обучения. По умолчанию равно 500.\n","\n","    Returns:\n","    -------\n","    tuple\n","        Возвращает кортеж, содержащий следующие элементы:\n","        - r2_train (float): Коэффициент детерминации R^2 на обучающем наборе данных.\n","        - r2_val (float): Коэффициент детерминации R^2 на валидационном наборе данных.\n","        - last_train_loss (float): Значение функции потерь на последней итерации обучения.\n","        - val_loss (float): Значение функции потерь на валидационном наборе данных.\n","        - loss_history (list[float]): История значений функции потерь в процессе обучения.\n","\n","    Примечания:\n","    ----------\n","    - Предполагается, что `X_train_array`, `Y_train_array`, `X_val_array`, `y_val_array` \n","      являются глобально определенными переменными и содержат данные для обучения и валидации модели.\n","    - Метод `fit` модели `LinearRegression` использует `descent_config` для конфигурации выбранного метода градиентного спуска.\n","    \"\"\"\n","    \n","    if descent_reg:\n","        if descent_name == 'stochastic':\n","            descent_config = {\n","                    'descent_name': descent_name,\n","                    'regularized': descent_reg, \n","                    'kwargs': {\n","                        'dimension': X_train.shape[1],                    \n","                        'batch_size': batch_size,\n","                        'lambda_' : lr,                    \n","                        'mu': mu,\n","                        'isBasis': isBasis\n","                    }\n","                }\n","        else:\n","            descent_config = {\n","                        'descent_name': descent_name,\n","                        'regularized': descent_reg, \n","                        'kwargs': {\n","                            'dimension': X_train.shape[1],                                           \n","                            'lambda_' : lr,                        \n","                            'mu': mu,\n","                            'isBasis': isBasis\n","                        }\n","                    }\n","    else:\n","        if descent_name == 'stochastic':\n","            descent_config = {\n","                    'descent_name': descent_name,\n","                    'regularized': descent_reg, \n","                    'kwargs': {\n","                        'dimension': X_train.shape[1],                    \n","                        'batch_size': batch_size,\n","                        'lambda_' : lr,\n","                        'isBasis': isBasis,\n","                        'loss_function': loss_function\n","                    }\n","                }\n","        else:\n","            descent_config = {\n","                        'descent_name': descent_name,\n","                        'regularized': descent_reg, \n","                        'kwargs': {\n","                            'dimension': X_train.shape[1],                                           \n","                            'lambda_' : lr,\n","                            'isBasis': isBasis,\n","                            'loss_function': loss_function\n","                        }\n","                    }\n","\n","    regression = LinearRegression(\n","            descent_config=descent_config,\n","            tolerance=tolerance,\n","            max_iter=max_iter,\n","            min_loss=min_loss\n","        )\n","\n","    regression.fit(X_train_array, Y_train_array)\n","\n","    r2_train = r2_score(Y_train_array, regression.predict(X_train_array))\n","    r2_val = r2_score(y_val, regression.predict(X_val_array))\n","    last_train_loss = regression.loss_history[-1]\n","    val_loss = regression.calc_loss(X_val_array, y_val_array)\n","    loss_history = regression.loss_history    \n","    \n","    return r2_train, r2_val, last_train_loss, val_loss, loss_history\n","\n","def start_lr_seach(lambda_values, descent_name: str = 'adam', descent_reg: bool = False, isBasis: bool = True, batch_size: int = None, tolerance: float = 1e-4, \n","                   max_iter: int = 300, min_loss: float = 0) -> Tuple[List[Dict[str, List[float]]], List[List[float]]]:\n","    \"\"\"\n","    Выполняет поиск по сетке скоростей обучения (learning rates) для оценки их влияния на обучение модели.\n","\n","    Parameters:\n","    ----------\n","    lambda_values : iterable\n","        Перечислимый объект (например, список или массив), содержащий значения скорости обучения для тестирования.\n","    descent_name : str, optional\n","        Название метода градиентного спуска. Поддерживаются 'adam' и 'stochastic'. По умолчанию 'adam'.\n","    batch_size : int, optional\n","        Размер батча для стохастического градиентного спуска. Только если descent_name='stochastic'. По умолчанию None.\n","    tolerance : float, optional\n","        Критерий остановки для квадрата евклидова нормы разности весов. По умолчанию равен 1e-4.\n","    max_iter : int, optional\n","        Максимальное количество итераций обучения. По умолчанию равно 500.\n","\n","    Returns:\n","    -------\n","    tuple\n","        Возвращает кортеж из двух элементов:\n","        - Первый элемент (list of dict): Список словарей, где каждый словарь содержит пару ключей 'lr' (значение скорости обучения) \n","          и 'loss_history' (список значений функции потерь для данного значения скорости обучения).\n","        - Второй элемент (list of list): Список списков, где каждый внутренний список содержит результаты для одного значения \n","          скорости обучения, включая само значение скорости обучения, значение функции потерь на последней итерации обучения, \n","          значение функции потерь на валидационном наборе данных, коэффициенты детерминации R^2 на обучающем и валидационном \n","          наборах данных, и максимальное количество итераций.\n","\n","    Пример использования:\n","    --------------------\n","    lambda_values = [0.001, 0.01, 0.1]\n","    list_of_dicts, results = start_lr_search(lambda_values, descent_name='adam', max_iter=500)\n","    \n","    # Далее, вы можете анализировать 'list_of_dicts' и 'results' для определения оптимального значения скорости обучения.\n","    \"\"\"\n","\n","    result = []\n","    lrs = []\n","    loss_histories = []\n","\n","    for lr in lambda_values:\n","        r2_train, r2_val, last_train_loss, val_loss, loss_history = get_train_result(lr=lr, descent_name=descent_name, descent_reg=descent_reg, \n","                                                                                     isBasis=isBasis,\n","                                                                                     batch_size=batch_size, tolerance=tolerance, \n","                                                                                     max_iter=max_iter, min_loss=min_loss)  \n","\n","        iter_count = len(loss_history)\n","        result.append([lr, last_train_loss, val_loss, r2_train, r2_val, iter_count])  \n","        lrs.append(lr)\n","        loss_histories.append(loss_history)\n","\n","        print_params(lr, last_train_loss, val_loss, r2_train, r2_val, iter_count)\n","\n","    list_of_dictionaries = [{\"lr\": lr, \"loss_history\": loss_history} for lr, loss_history in zip(lrs, loss_histories)]\n","\n","    df_result = pd.DataFrame(result)\n","    df_result.columns = ['Learning_Rate', 'Error_Trein', 'Error_Val', 'R2_Train', 'R2_Val', 'Iter_count']\n","\n","    return list_of_dictionaries, df_result\n","\n","def start_lr_and_mu_seach(lambda_values, mu_valuse, descent_name: str = 'adam', descent_reg: bool = False, isBasis: bool = True, \n","                          batch_size: int = None, tolerance: float = 1e-4, max_iter: int = 300) -> Tuple[List[Dict[str, List[float]]], List[List[float]]]:\n","    \"\"\"\n","    Выполняет поиск по сетке скоростей обучения (learning rates) для оценки их влияния на обучение модели.\n","\n","    Parameters:\n","    ----------\n","    lambda_values : iterable\n","        Перечислимый объект (например, список или массив), содержащий значения скорости обучения для тестирования.\n","    descent_name : str, optional\n","        Название метода градиентного спуска. Поддерживаются 'adam' и 'stochastic'. По умолчанию 'adam'.\n","    batch_size : int, optional\n","        Размер батча для стохастического градиентного спуска. Только если descent_name='stochastic'. По умолчанию None.\n","    tolerance : float, optional\n","        Критерий остановки для квадрата евклидова нормы разности весов. По умолчанию равен 1e-4.\n","    max_iter : int, optional\n","        Максимальное количество итераций обучения. По умолчанию равно 500.\n","\n","    Returns:\n","    -------\n","    tuple\n","        Возвращает кортеж из двух элементов:\n","        - Первый элемент (list of dict): Список словарей, где каждый словарь содержит пару ключей 'lr' (значение скорости обучения) \n","          и 'loss_history' (список значений функции потерь для данного значения скорости обучения).\n","        - Второй элемент (list of list): Список списков, где каждый внутренний список содержит результаты для одного значения \n","          скорости обучения, включая само значение скорости обучения, значение функции потерь на последней итерации обучения, \n","          значение функции потерь на валидационном наборе данных, коэффициенты детерминации R^2 на обучающем и валидационном \n","          наборах данных, и максимальное количество итераций.\n","\n","    Пример использования:\n","    --------------------\n","    lambda_values = [0.001, 0.01, 0.1]\n","    list_of_dicts, results = start_lr_search(lambda_values, descent_name='adam', max_iter=500)\n","    \n","    # Далее, вы можете анализировать 'list_of_dicts' и 'results' для определения оптимального значения скорости обучения.\n","    \"\"\"\n","\n","    result = []\n","    lrs = []\n","    mus = []\n","    loss_histories = []\n","\n","    for lr in lambda_values:\n","        for mu in mu_valuse:\n","\n","            r2_train, r2_val, last_train_loss, val_loss, loss_history = get_train_result(lr=lr, descent_name=descent_name, descent_reg=descent_reg, isBasis=isBasis,\n","                                                                                        mu=mu, batch_size=batch_size, tolerance=tolerance, max_iter=max_iter)  \n","\n","            result.append([lr, mu, last_train_loss, val_loss, r2_train, r2_val, max_iter])  \n","            lrs.append(lr)\n","            loss_histories.append(loss_history)\n","            mus.append(mu)\n","            list_of_dictionaries = [{\"lr\": lr, \"mu\": mu, \"loss_history\": loss_history} for lr, mu, loss_history in zip(lrs, mus, loss_histories)]\n","            iter_count = len(loss_history)\n","            \n","            print_params(lr, last_train_loss, val_loss, r2_train, r2_val, iter_count)\n","\n","            df_result = pd.DataFrame(result)\n","            df_result.columns = ['Learning_Rate', 'Mu', 'Error_Trein', 'Error_Val', 'R2_Train', 'R2_Val', 'Iter_count']\n","\n","    return list_of_dictionaries, df_result "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Анализ сходимости Full градиентного спуска\n","\n","В ходе продолжительных испытаний на некоторых параметрах lr сходимость не достигается за разумное время. По этому ограничимся 500 эпохами, и значением в 0.35 для параметра ошибки модели. Если за 500 итераций значение выше порогового, то будем сяитать что сходимость не достигается за разумное время и эти параметры не могут быть оптемальными. Если ошибка меньше 0.35 то считаем что сходимость достигнута."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#\n","full_lr_loss_history, full_df = start_lr_seach(lambda_values=lambda_values, descent_name='full', min_loss=0.35)\n","\n","# Строим график зависимости lr от error\n","\n","def plot_err_vs_lr(df: pd.DataFrame, descent_name: str = 'full'):\n","\n","    plt.figure(figsize=(10, 6))\n","    sns.lineplot(x='Learning_Rate', y='Error_Val', data=df, label='Validation Error')\n","    sns.lineplot(x='Learning_Rate', y='Error_Trein', data=df, label='Training Error')\n","    plt.xlabel('Learning Rate')\n","    plt.ylabel('Error')\n","    title = descent_name + ' Error vs Learning Rate'\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","plot_err_vs_lr(full_df, descent_name='Full')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.set_option('display.float_format', lambda d: '%.5f' % d)\n","full_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Создаем отдельный график для каждого значения lr\n","def plot_loss_history(loss_history_by_descent, descent_name: str = 'adam'):\n","\n","    for dictionary in loss_history_by_descent:\n","        lr = dictionary[\"lr\"]\n","        loss_history = dictionary[\"loss_history\"]\n","        \n","        # Создаем новый график\n","        plt.figure(figsize=(8, 4))\n","        plt.plot(loss_history, marker='o', linestyle='-')\n","        \n","        # Добавляем заголовок, включающий значение lr\n","        plt.title(f'{descent_name} loss history for Learning Rate : {lr}')\n","        \n","        # Называем оси\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        \n","        # Отображаем график\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_loss_history(loss_history_by_descent=full_lr_loss_history, descent_name='Full')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Из графиков видно, что оптемальное значение lr находится где то в районе 0.06. Попробуем подобрать его в ручную, как и количество необходимых итераций для более глубокой сходимости."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_lr = 0.03162\n","full_r2_train, full_r2_val, full_last_train_loss, full_val_loss, full_loss_history = get_train_result(lr=full_lr, descent_name='full', max_iter=iter_count, min_loss=0.26)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_optimal_loss_history(loss_history):\n","    # Выводим историю ошибок на график\n","    plt.figure(figsize=(8, 4))\n","    plt.plot(loss_history)\n","\n","    plt.figure(figsize=(8, 4))    \n","    filtered_regression_history= [val for val in loss_history if val < 1]\n","    # Создаем график для отфильтрованных значений\n","    plt.plot(filtered_regression_history)\n","\n","plot_optimal_loss_history(full_loss_history)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Анализ StochasticDescent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stochastic_lr_loss_history, stochastic_df = start_lr_seach(lambda_values=lambda_values, descent_name='stochastic', batch_size=batch_size, min_loss=0.35)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_err_vs_lr(stochastic_df, descent_name='Stochastic')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_loss_history(loss_history_by_descent=stochastic_lr_loss_history, descent_name='Stochastic')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stochastic_df"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'get_train_result' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stochastic_lr \u001b[39m=\u001b[39m \u001b[39m0.03162\u001b[39m\n\u001b[1;32m----> 2\u001b[0m stochastic_r2_train, stochastic_r2_val, stochastic_last_train_loss, stochastic_val_loss, stochastic_loss_history \u001b[39m=\u001b[39m get_train_result(lr\u001b[39m=\u001b[39mstochastic_lr, descent_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstochastic\u001b[39m\u001b[39m'\u001b[39m, max_iter\u001b[39m=\u001b[39miter_count, batch_size\u001b[39m=\u001b[39mbatch_size, min_loss\u001b[39m=\u001b[39m\u001b[39m0.26\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'get_train_result' is not defined"]}],"source":["stochastic_lr = 0.03162\n","stochastic_r2_train, stochastic_r2_val, stochastic_last_train_loss, stochastic_val_loss, stochastic_loss_history = get_train_result(lr=stochastic_lr, descent_name='stochastic', max_iter=iter_count, batch_size=batch_size, min_loss=0.26)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(stochastic_lr, stochastic_last_train_loss, stochastic_val_loss, stochastic_r2_train, stochastic_r2_val, len(stochastic_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_optimal_loss_history(stochastic_loss_history)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Momentum"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["momentum_lr_loss_history, momentum_df = start_lr_seach(lambda_values=lambda_values, descent_name='momentum')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_loss_history(momentum_lr_loss_history, 'Momentum')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_err_vs_lr(momentum_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["momentum_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["momentum_lr = 0.03162\n","momentum_r2_train, momentum_r2_val, momentum_last_train_loss, momentum_val_loss, momentum_loss_history = get_train_result(lr=momentum_lr, descent_name='momentum', max_iter=iter_count, min_loss=0.26)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(momentum_lr, momentum_last_train_loss, momentum_val_loss, momentum_r2_train, momentum_r2_val, len(momentum_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_optimal_loss_history(momentum_loss_history)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Adam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adam_lr_loss_history, adam_df = start_lr_seach(lambda_values=lambda_values, descent_name='adam')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_loss_history(adam_lr_loss_history, descent_name='Adam')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_err_vs_lr(adam_df, descent_name='Adam')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adam_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adam_lr = 0.56234\n","adam_r2_train, adam_r2_val, adam_last_train_loss, adam_val_loss, adam_loss_history = get_train_result(lr=adam_lr, descent_name='adam', max_iter=iter_count, min_loss=0.26)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(adam_lr, adam_last_train_loss, adam_val_loss, adam_r2_train, adam_r2_val, len(adam_loss_history))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X8QqbdVzT5Ch","pycharm":{"name":"#%% md\n"}},"source":["### Задание 5.2. Сравнение методов (0.5 балла)\n","\n","Создайте график, на котором будет показана динамика изменения ошибки на обучающей выборке в зависимости от номера итерации для каждого из рассматриваемых методов. Разместите линии, представляющие каждый метод, на одном и том же графике для наглядного сравнения.\n","\n","После анализа результатов, представленных в виде графика и таблиц с метриками, выполните сравнение методов. Обратите внимание на следующие аспекты:\n","- Как быстро каждый метод сходится к минимуму ошибки.\n","- Величину ошибки на обучающей и тестовой выборках для каждого метода.\n","- Значение метрики $R^2$ для каждого метода на обучающей и тестовой выборках.\n","- Количество итераций, необходимых для достижения сходимости.\n","\n","На основе этих данных сделайте вывод о преимуществах и недостатках каждого из методов в контексте вашей задачи."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_optimal_loss_history(full_loss_history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_optimal_loss_history(stochastic_loss_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0m7KtlWMT5Ch","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["plot_optimal_loss_history(momentum_loss_history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_optimal_loss_history(adam_loss_history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(full_lr, full_last_train_loss, full_val_loss, full_r2_train, full_r2_val, len(full_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(stochastic_lr, stochastic_last_train_loss, stochastic_val_loss, stochastic_r2_train, stochastic_r2_val, len(stochastic_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(momentum_lr, momentum_last_train_loss, momentum_val_loss, momentum_r2_train, momentum_r2_val, len(momentum_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(adam_lr, adam_last_train_loss, adam_val_loss, adam_r2_train, adam_r2_val, len(adam_loss_history))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iboLWHS_T5Ch","pycharm":{"name":"#%% md\n"}},"source":["Выше представленны результаты работы 4х различных методов градиентного спуска. Быстрей всего до минимального значения MSE сходится Adam. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"POJZrXzDT5Ch","pycharm":{"name":"#%% md\n"}},"source":["## Задание 6. Стохастический градиентный спуск и размер батча (0.5 балла)\n","\n","Ваша задача — исследовать, как размер батча влияет на процесс обучения при использовании стохастического градиентного спуска (SGD). Выполните следующие шаги:\n","\n","1. **Выбор размеров батча**: Определите ряд значений для размера батча, которые вы хотите исследовать. Это могут быть, например, %1$, $10$, $50$, $100$, $500$, и так далее.\n","\n","2. **Многократные запуски для каждого размера батча**: Для каждого выбранного размера батча проведите $k$ независимых запусков стохастического градиентного спуска на обучающей выборке. $k$ может быть равно, например, $10$. Для каждого запуска замерьте:\n","   - Время обучения в секундах до достижения сходимости.\n","   - Количество итераций (шагов), необходимых для сходимости.\n","\n","3. **Вычисление средних значений**: Рассчитайте среднее время обучения и среднее количество итераций до сходимости для каждого размера батча.\n","\n","4. **Построение графиков**:\n","   - Постройте график, показывающий зависимость среднего количества итераций до сходимости от размера батча.\n","   - Постройте график, показывающий зависимость среднего времени обучения от размера батча.\n","\n","5. **Анализ результатов**: Оцените, как размер батча влияет на скорость и эффективность обучения. Сделайте выводы о том, какой размер батча может быть оптимальным с точки зрения баланса между временем обучения и количеством итераций до сходимости.\n","\n","Этот эксперимент поможет вам лучше понять влияние размера батча на процесс обучения стохастического градиентного спуска и как этот параметр можно настроить для улучшения производительности обучения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-PhWFVET5Ci","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import time\n","\n","from matplotlib.pyplot import tick_params\n","\n","batch_sizes = np.arange(10, 1000, 100)\n","columns = ['batch_size', 'time_result', 'iter_count', 'r2_train', 'r2_val', 'last_train_loss', 'val_loss']\n","\n","sgd_res_df = pd.DataFrame(columns=columns)\n","\n","for batch in batch_sizes:\n","\n","    \n","\n","    for k in range(10):\n","        start_time = time.perf_counter()  \n","\n","        r2_train, r2_val, last_train_loss, val_loss, loss_history =  get_train_result(lr=stochastic_lr, descent_name='stochastic', batch_size=batch, min_loss=0.335)\n","\n","        end_time = time.perf_counter() \n","        time_result = end_time - start_time\n","\n","        temp_dict = {\n","            'batch_size': batch, \n","            'time_result': time_result, \n","            'iter_count': len(loss_history), \n","            'r2_train': r2_train, \n","            'r2_val': r2_val, \n","            'last_train_loss': last_train_loss, \n","            'val_loss': val_loss\n","        }\n","\n","        print(temp_dict)\n","        # Превращаем словарь в строку DataFrame и добавляем к основному DataFrame\n","        # Создаём временный DataFrame из словаря\n","        temp_df = pd.DataFrame([temp_dict])\n","        \n","        # Используем pd.concat для добавления строки\n","        sgd_res_df = pd.concat([sgd_res_df, temp_df], ignore_index=True)  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sgd_res_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grouped_loss = sgd_res_df.groupby('batch_size')['last_train_loss'].mean().reset_index()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","plt.plot(grouped_loss['batch_size'], grouped_loss['last_train_loss'], marker='o', linestyle='-', color='b')\n","plt.title('Зависимость среднего количества итераций до сходимости от размера батча')\n","plt.xlabel('Размер батча')\n","plt.ylabel('Средняя ошибка для указанного бача')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Группируем по размеру батча и считаем среднее количество итераций\n","grouped_iter = sgd_res_df.groupby('batch_size')['iter_count'].mean().reset_index()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","plt.plot(grouped_iter['batch_size'], grouped_iter['iter_count'], marker='o', linestyle='-', color='b')\n","plt.title('Зависимость среднего количества итераций до сходимости от размера батча')\n","plt.xlabel('Размер батча')\n","plt.ylabel('Среднее количество итераций до сходимости')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grouped_iter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Группируем по размеру батча и считаем среднее количество итераций\n","grouped_time = sgd_res_df.groupby('batch_size')['time_result'].mean().reset_index()\n","\n","# Строим график\n","plt.figure(figsize=(10, 6))\n","plt.plot(grouped_time['batch_size'], grouped_time['time_result'], marker='o', linestyle='-', color='b')\n","plt.title('Зависимость среднего времени до сходимости от размера батча')\n","plt.xlabel('Размер батча')\n","plt.ylabel('Среднее время до сходимости')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grouped_time"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YUVZvNuJT5Ci","pycharm":{"name":"#%% md\n"}},"source":["Исходя из результатов по времени и количеству итераций опримальный размер бача находится в районе 100."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"N6nZS55_T5Cj","pycharm":{"name":"#%% md\n"}},"source":["## Задание 7. Регуляризация (0.5 балла)\n","\n","В этом задании вам предстоит исследовать влияние регуляризации на работу различных методов градиентного спуска. Напомним, регуляризация - это добавка к функции потерь, которая штрафует за норму весов. Мы будем использовать l2 регуляризацию, таким образом функция потерь приобретает следующий вид:\n","\n","$$\n","    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2 + \\dfrac{\\mu}{2} \\| w \\|^2\n","$$"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"W3jbvy89T5Cj","pycharm":{"name":"#%% md\n"}},"source":["Допишите класс **BaseDescentReg** в файле `descents.py`."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j1E7bAjUT5Cj","pycharm":{"name":"#%% md\n"}},"source":["Найдите лучшие параметры обучения с регуляризацией аналогично 5 заданию. Вам предстоит исследовать, как настройка параметров обучения с включением регуляризации влияет на различные методы градиентного спуска. Основная цель — определить оптимальные значения для длины шага $\\lambda$ и коэффициента регуляризации $\\mu$, а затем сравнить результаты обучения с регуляризацией и без неё по нескольким критериям.\n","\n","Ваш план действий следующий:\n","\n","1. **Выбор параметров для подбора**: Установите диапазон значений для длины шага $\\lambda$ и коэффициента регуляризации $\\mu$. Используйте логарифмическую сетку для обоих параметров, чтобы обеспечить широкий охват потенциально оптимальных значений.\n","\n","2. **Оптимизация и сравнение методов градиентного спуска**:\n","   - Произведите подбор параметров для каждого метода градиентного спуска, исследуя их влияние на процесс обучения.\n","   - Замерьте и сравните ошибку и качество по метрике $R^2$ на обучающей и тестовой выборках, а также количество итераций до сходимости для моделей с регуляризацией и без неё.\n","\n","3. **Визуализация результатов**:\n","   - Постройте для каждого метода графики, отображающие значения функции потерь (MSE) с регуляризацией и без неё на протяжении процесса обучения.\n","\n","4. **Анализ результатов**:\n","   - Оцените, как регуляризация повлияла на сходимость методов.\n","   - Сравните качество моделей на обучающей и тестовой выборках с учетом регуляризации и без неё.\n","   - Проанализируйте, как изменения в длине шага и коэффициенте регуляризации отразились на итоговых результатах.\n","\n","5. **Формулировка выводов**:\n","   - Сделайте выводы о влиянии регуляризации на процесс обучения и качество модели. Как регуляризация влияет на переобучение и обобщающую способность модели.\n","   - Рассмотрите, в каких случаях регуляризация приводит к улучшению результатов, и когда её вклад может быть минимальным или отрицательным.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pA762KhbT5Cj","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["mu_values = np.logspace(-4, 1, num=5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Full mu и lr анализ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_mu_and_lr_loss, full_mu_and_lr_df = start_lr_and_mu_seach(lambda_values=lambda_values, mu_valuse=mu_values, descent_name='full', descent_reg=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_mu_and_lr_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","def plot_3d_loss_mu_lr(list_of_dictionaries):\n","    # Первоначальный код для получения исходных данных\n","    lrs = [d['lr'] for d in list_of_dictionaries]\n","    mus = [d['mu'] for d in list_of_dictionaries]\n","    last_losses = [d['loss_history'][-1] for d in list_of_dictionaries]\n","    last_losses = np.clip(last_losses, None, 1)\n","    lrs = np.array(lrs)\n","    mus = np.array(mus)\n","    last_losses = np.array(last_losses)\n","\n","    # Построение первого графика\n","    fig = plt.figure(figsize=(14, 6))\n","\n","    # Первый график\n","    ax1 = fig.add_subplot(121, projection='3d')\n","    scatter1 = ax1.scatter(lrs, mus, last_losses, c=last_losses, cmap='viridis')\n","    ax1.set_xlabel('Learning Rate (lr)')\n","    ax1.set_ylabel('Regularization Coefficient (mu)')\n","    ax1.set_zlabel('Last Loss')\n","    ax1.set_title('Original Data')\n","    fig.colorbar(scatter1, shrink=0.5, aspect=5)\n","\n","    # Ограничение lr и mu значениями до 0.05\n","    lrs_clipped = np.clip(lrs, None, 0.05)\n","    mus_clipped = np.clip(mus, None, 0.05)\n","\n","    # Второй график\n","    ax2 = fig.add_subplot(122, projection='3d')\n","    scatter2 = ax2.scatter(lrs_clipped, mus_clipped, last_losses, c=last_losses, cmap='viridis')\n","    ax2.set_xlabel('Learning Rate (lr)')\n","    ax2.set_ylabel('Regularization Coefficient (mu)')\n","    ax2.set_zlabel('Last Loss')\n","    ax2.set_title('Clipped Data (lr, mu <= 0.05)')\n","    fig.colorbar(scatter2, shrink=0.5, aspect=5)\n","\n","    plt.tight_layout()\n","    plt.show()\n","    \n","plot_3d_loss_mu_lr(full_mu_and_lr_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_mu_lr = 0.03162\n","full_mu = 0.03162"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_mu_r2_train, full_mu_r2_val, full_mu_last_train_loss, full_mu_val_loss, full_mu_loss_history = get_train_result(lr=full_mu_lr, descent_name='full', descent_reg=True, mu=full_mu, max_iter=iter_count, min_loss=0.26)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(full_mu_lr, full_mu_last_train_loss, full_mu_val_loss, full_mu_r2_train, full_mu_r2_val, iter_count=iter_count, mu=full_mu)\n","print_params(full_lr, full_last_train_loss, full_val_loss, full_r2_train, full_r2_val, iter_count=iter_count)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_error_lines(errors, errors_mu, start_val: int = 100):\n","    \"\"\"\n","    Функция для отображения двух линий ошибок на графике, а также второго графика\n","    с данными начиная с 100-й итерации.\n","\n","    Параметры:\n","    - errors: Список значений ошибок для метода без регуляризации.\n","    - errors_mu: Список значений ошибок для метода с регуляризацией.\n","    \"\"\"\n","    \n","    # Создаем фигуру и два подграфика (subplots)\n","    fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n","    \n","    # Первый подграфик для всех данных\n","    axs[0].plot(range(1, len(errors) + 1), errors, label='Ошибки', marker='o')\n","    axs[0].plot(range(1, len(errors_mu) + 1), errors_mu, label='Ошибки с регуляризацией', marker='s')\n","    axs[0].set_title('График всех ошибок')\n","    axs[0].set_xlabel('Номер итерации')\n","    axs[0].set_ylabel('Значение ошибки')\n","    axs[0].legend()\n","\n","    \n","    axs[1].plot(range(start_val + 1, len(errors) + 1), errors[start_val:], label='Ошибки', marker='o')\n","    axs[1].plot(range(start_val + 1, len(errors_mu) + 1), errors_mu[start_val:], label='Ошибки с регуляризацией', marker='s')\n","    axs[1].set_title('График ошибок с {}-й итерации'.format(start_val))\n","    axs[1].set_xlabel('Номер итерации')\n","    axs[1].set_ylabel('Значение ошибки')\n","    axs[1].legend()\n","\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_error_lines(full_loss_history, full_mu_loss_history)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Stochastic mu lr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stochastic_mu_and_lr_loss, stochastic_mu_and_lr_df = start_lr_and_mu_seach(lambda_values=lambda_values, mu_valuse=mu_values, descent_name='stochastic', \n","                                                                           descent_reg=True, batch_size=batch_size)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hHFWz8RXT5Ck","pycharm":{"name":"#%% md\n"}},"source":["╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_3d_loss_mu_lr(stochastic_mu_and_lr_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stochastic_mu_and_lr_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stochastic_mu_lr = 0.03162\n","stochastic_mu = 0.03162"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","stochastic_mu_r2_train, stochastic_mu_r2_val, stochastic_mu_last_train_loss, stochastic_mu_val_loss, stochastic_mu_loss_history = get_train_result(lr=stochastic_mu_lr, descent_name='stochastic', descent_reg=True, mu=stochastic_mu, batch_size=batch_size, max_iter=iter_count, min_loss=0.26)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(stochastic_mu_lr, stochastic_mu_last_train_loss, stochastic_mu_val_loss, stochastic_mu_r2_train, stochastic_mu_r2_val, \n","             iter_count=len(stochastic_mu_loss_history), mu=stochastic_mu, batch_sizes=batch_size)\n","print_params(stochastic_lr, stochastic_last_train_loss, stochastic_val_loss, stochastic_r2_train, stochastic_r2_val, iter_count=len(stochastic_loss_history), batch_sizes=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_error_lines(stochastic_loss_history, stochastic_mu_loss_history, 50)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Momentum lr mu  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["momentum_mu_and_lr_loss, momentum_mu_and_lr_df = start_lr_and_mu_seach(lambda_values=lambda_values, mu_valuse=mu_values, descent_name='momentum', descent_reg=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["momentum_mu_and_lr_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_3d_loss_mu_lr(momentum_mu_and_lr_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["momentum_mu_lr = 0.03162\n","momentum_mu = 0.03162"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","momentum_mu_r2_train, momentum_mu_r2_val, momentum_mu_last_train_loss, momentum_mu_val_loss, momentum_mu_loss_history = get_train_result(lr=momentum_mu_lr, descent_name='momentum', descent_reg=True, mu=momentum_mu, max_iter=iter_count, min_loss=0.26)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Печать параметров для градиентного спуска с моментом и регуляризацией\n","print_params(momentum_mu_lr, momentum_mu_last_train_loss, momentum_mu_val_loss, \n","             momentum_mu_r2_train, momentum_mu_r2_val, iter_count=len(momentum_mu_loss_history), \n","             mu=momentum_mu)\n","\n","# Печать параметров для градиентного спуска с моментом без регуляризации\n","print_params(momentum_lr, momentum_last_train_loss, momentum_val_loss, \n","             momentum_r2_train, momentum_r2_val, iter_count=len(momentum_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_error_lines(momentum_loss_history, momentum_mu_loss_history, 50)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Adam lr mu"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adam_mu_and_lr_loss, adam_mu_and_lr_df = start_lr_and_mu_seach(lambda_values=lambda_values, mu_valuse=mu_values, descent_name='adam', descent_reg=True) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adam_mu_and_lr_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_3d_loss_mu_lr(adam_mu_and_lr_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adam_mu_lr = 0.56234\n","adam_mu = 0.00010\n","iter_count = 500"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","adam_mu_r2_train, adam_mu_r2_val, adam_mu_last_train_loss, adam_mu_val_loss, adam_mu_loss_history = get_train_result(lr=adam_mu_lr, descent_name='adam', descent_reg=True, mu=adam_mu, max_iter=iter_count, min_loss=0.257)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Печать параметров для градиентного спуска с Adam и регуляризацией\n","print_params(adam_mu_lr, adam_mu_last_train_loss, adam_mu_val_loss, \n","             adam_mu_r2_train, adam_mu_r2_val, iter_count=len(adam_mu_loss_history), \n","             mu=adam_mu)\n","\n","# Печать параметров для градиентного спуска с Adam без регуляризации\n","print_params(adam_lr, adam_last_train_loss, adam_val_loss, \n","             adam_r2_train, adam_r2_val, iter_count=len(adam_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_error_lines(adam_loss_history, adam_mu_loss_history, 50)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uv5IIxaQT5Ck","pycharm":{"name":"#%% md\n"}},"source":["## Задание 8. Альтернативная функция потерь (0.5 балла)\n","\n","В этом задании вам предстоит использовать другую функцию потерь для нашей задачи регрессии. В качестве функции потерь мы выбрали **Log-Cosh**:\n","\n","$$\n","    L(y, a)\n","    =\n","    \\log({cosh{(a - y)}}).\n","$$\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mzibZ5S2T5Ck","pycharm":{"name":"#%% md\n"}},"source":["Самостоятельно продифференцируйте данную функцию потерь чтобы найти её градиент:\n","\n","Давайте найдем производную функции потерь $L(y, a)$ по $a$, используя данную формулу:\n","$$\n","    L(y, a)\n","    =\n","    \\log(\\cosh(a - y)).\n","$$\n","\n","Шаги дифференцирования:\n","\n","1. **Найдем внешнюю производную** функции относительно $a$, используя цепное правило. Внешняя функция здесь — это $\\log(z)$, где $z = \\cosh(a - y)$. Производная $\\log(z)$ по $z$ равна $\\frac{1}{z}$.\n","   \n","   Таким образом, внешняя производная будет:\n","   $$\n","   \\frac{d}{dz} \\log(z) = \\frac{1}{z} = \\frac{1}{\\cosh(a - y)}.\n","   $$\n","\n","2. **Найдем внутреннюю производную** функции относительно $a$. Внутренняя функция здесь — это $\\cosh(a - y)$, где $a - y$ является аргументом функции косинуса гиперболического.\n","\n","   Производная $\\cosh(x)$ по $x$ равна $\\sinh(x)$. Поэтому, производная $\\cosh(a - y)$ по $a$ равна $\\sinh(a - y)$.\n","\n","3. **Применим цепное правило**, умножив внешнюю производную на внутреннюю. Итак, производная $L(y, a)$ по $a$ равна:\n","   $$\n","   \\frac{dL}{da} = \\frac{1}{\\cosh(a - y)} \\cdot \\sinh(a - y).\n","   $$\n","\n","4. **Упрощение выражения**. Данное выражение уже достаточно простое и представляет собой итоговую производную функции потерь по $a$.\n","\n","Итак, производная функции потерь $L(y, a)$ по $a$ равна:\n","$$\n","\\frac{dL}{da} = \\frac{\\sinh(a - y)}{\\cosh(a - y)}.\n","$$\n","\n","Это выражение также можно упростить, зная, что $\\frac{\\sinh(x)}{\\cosh(x)} = \\tanh(x)$, получим:\n","$$\n","\\frac{dL}{da} = \\tanh(a - y).\n","$$\n","\n","Это окончательный результат дифференцирования заданной функции потерь."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E2eid0ztT5Ck","pycharm":{"name":"#%% md\n"}},"source":["Программно реализуйте градиентный спуск с данной функцией потерь в файле `descents.py`, обучите все четыре метода (без регуляризации) аналогично 5 заданию, сравните их качество с четырьмя методами из 5 задания.\n","\n","Пример того, как можно запрограммировать использование нескольких функций потерь внутри одного класса градиентного спуска:\n","\n","\n","```python\n","from enum import auto\n","from enum import Enum\n","\n","import numpy as np\n","\n","class LossFunction(Enum):\n","    MSE = auto()\n","    MAE = auto()\n","    LogCosh = auto()\n","    Huber = auto()\n","\n","...\n","class BaseDescent:\n","    def __init__(self, loss_function: LossFunction = LossFunction.MSE):\n","        self.loss_function: LossFunction = loss_function\n","\n","    def calc_gradient(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n","        if self.loss_function is LossFunction.MSE:\n","            return ...\n","        elif self.loss_function is LossFunction.LogCosh:\n","            return ...\n","...\n","\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zxUsZ5OT5Ck","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["adam_logcosh_r2_train, adam_logcosh_r2_val, adam_logcosh_last_train_loss, adam_logcosh_val_loss, adam_logcosh_loss_history = get_train_result(loss_function=LossFunction.LogCosh, \n","                                                                                                                     lr=adam_mu_lr, descent_name='adam')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Печать параметров для градиентного спуска с Adam с функцией потель LogCosh\n","print_params(adam_mu_lr, adam_logcosh_last_train_loss, adam_logcosh_val_loss, \n","             adam_logcosh_r2_train, adam_logcosh_r2_val, iter_count=len(adam_logcosh_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Печать параметров для градиентного спуска с Adam\n","print_params(adam_lr, adam_last_train_loss, adam_val_loss, \n","             adam_r2_train, adam_r2_val, iter_count=len(adam_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["momentum_logcosh_r2_train, momentum_logcosh_r2_val, momentum_logcosh_last_train_loss, momentum_logcosh_val_loss, momentum_logcosh_loss_history = get_train_result(\n","    loss_function=LossFunction.LogCosh, lr=momentum_mu_lr, descent_name='momentum')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(momentum_mu_lr, momentum_logcosh_last_train_loss, momentum_logcosh_val_loss, \n","             momentum_logcosh_r2_train, momentum_logcosh_r2_val, iter_count=len(momentum_logcosh_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(momentum_lr, momentum_last_train_loss, momentum_val_loss, \n","             momentum_r2_train, momentum_r2_val, iter_count=len(momentum_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stochastic_logcosh_r2_train, stochastic_logcosh_r2_val, stochastic_logcosh_last_train_loss, stochastic_logcosh_val_loss, stochastic_logcosh_loss_history = get_train_result(\n","    loss_function=LossFunction.LogCosh, lr=stochastic_mu_lr, batch_size=batch_size,  descent_name='stochastic')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(stochastic_mu_lr, stochastic_logcosh_last_train_loss, stochastic_logcosh_val_loss, \n","             stochastic_logcosh_r2_train, stochastic_logcosh_r2_val, iter_count=len(stochastic_logcosh_loss_history))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_params(stochastic_lr, stochastic_last_train_loss, stochastic_val_loss, stochastic_r2_train, stochastic_r2_val, iter_count=len(stochastic_loss_history), batch_sizes=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_logcosh_r2_train, full_logcosh_r2_val, full_logcosh_last_train_loss, full_logcosh_val_loss, full_logcosh_loss_history = get_train_result(\n","    loss_function=LossFunction.LogCosh, lr=full_mu_lr, descent_name='full')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_comparison_errors(full_errors, stochastic_errors, momentum_errors, adam_errors, start_val: int = 50):\n","    \"\"\"\n","    Функция для отображения четырех линий ошибок на графике, представляющих различные\n","    методы оптимизации (полный градиентный спуск, стохастический градиентный спуск, \n","    градиентный спуск с моментумом и Adam), а также второго графика с данными начиная\n","    с указанной итерации.\n","\n","    Параметры:\n","    - full_errors: Список значений ошибок для полного градиентного спуска.\n","    - stochastic_errors: Список значений ошибок для стохастического градиентного спуска.\n","    - momentum_errors: Список значений ошибок для градиентного спуска с моментумом.\n","    - adam_errors: Список значений ошибок для оптимизатора Adam.\n","    - start_val: Итерация, с которой начинается второй график.\n","    \"\"\"\n","    \n","    # Создаем фигуру и два подграфика (subplots)\n","    fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n","    \n","    # Первый подграфик для всех данных\n","    axs[0].plot(range(1, len(full_errors) + 1), full_errors, label='Full Gradient Descent', marker='o')\n","    axs[0].plot(range(1, len(stochastic_errors) + 1), stochastic_errors, label='Stochastic Gradient Descent', marker='s')\n","    axs[0].plot(range(1, len(momentum_errors) + 1), momentum_errors, label='Momentum Gradient Descent', marker='^')\n","    axs[0].plot(range(1, len(adam_errors) + 1), adam_errors, label='Adam Optimizer', marker='x')\n","    axs[0].set_title('Comparison of Error Rates Across Methods')\n","    axs[0].set_xlabel('Iteration Number')\n","    axs[0].set_ylabel('Error Value')\n","    axs[0].legend()\n","\n","    # Второй подграфик с указанной итерации\n","    axs[1].plot(range(start_val + 1, len(full_errors) + 1), full_errors[start_val:], label='Full Gradient Descent', marker='o')\n","    axs[1].plot(range(start_val + 1, len(stochastic_errors) + 1), stochastic_errors[start_val:], label='Stochastic Gradient Descent', marker='s')\n","    axs[1].plot(range(start_val + 1, len(momentum_errors) + 1), momentum_errors[start_val:], label='Momentum Gradient Descent', marker='^')\n","    axs[1].plot(range(start_val + 1, len(adam_errors) + 1), adam_errors[start_val:], label='Adam Optimizer', marker='x')\n","    axs[1].set_title('Comparison from the {}-th Iteration Onwards'.format(start_val))\n","    axs[1].set_xlabel('Iteration Number')\n","    axs[1].set_ylabel('Error Value')\n","    axs[1].legend()\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_comparison_errors(full_logcosh_loss_history, stochastic_logcosh_loss_history, momentum_logcosh_loss_history, adam_logcosh_loss_history, 50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_comparison_errors(full_loss_history, stochastic_loss_history, momentum_loss_history, adam_loss_history, 50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_comparison_errors(full_mu_loss_history, stochastic_mu_loss_history, momentum_mu_loss_history, adam_mu_loss_history, 50)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"}}},"nbformat":4,"nbformat_minor":0}
